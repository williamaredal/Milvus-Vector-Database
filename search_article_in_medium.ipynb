{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332f97b2",
   "metadata": {},
   "source": [
    "# Searh article in Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db978d0",
   "metadata": {},
   "source": [
    "## 0. Overview\n",
    "\n",
    "We'll search for text in the Medium dataset, and it will find the most similar results to the search text across all titles. Searching for articles is different from traditional keyword searches, which search for semantically relevant content. If you search for \"**funny python demo**\" it will return \"**Python Coding for Kids - Setting Up For the Adventure**\", not \"**No key words about funny python demo**\".\n",
    "\n",
    "We will use Milvus and Towhee to help searches. Towhee is used to extract the semantics of the text and return the text embedding. The Milvus vector database can store and search vectors, and return related articles. So we first need to install [Milvus](https://github.com/milvus-io/milvus) and [Towhee](https://github.com/towhee-io/towhee).\n",
    "\n",
    "Before getting started, please make sure that you have started a [Milvus service](https://milvus.io/docs/install_standalone-docker.md). This notebook uses [milvus 2.2.10](https://milvus.io/docs/v2.2.x/install_standalone-docker.md) and [pymilvus 2.2.11](https://milvus.io/docs/release_notes.md#2210)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f85c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pymilvus\n",
      "Version: 2.1.1\n",
      "Name: towhee\n",
      "Version: 1.1.3\n"
     ]
    }
   ],
   "source": [
    "#! pip install --upgrade pip\n",
    "#! pip3 install -q towhee pymilvus==2.2.11\n",
    "#! pip3 uninstall pymilvus -y\n",
    "\n",
    "! pip3 install -q towhee pymilvus==2.1.1\n",
    "! pip3 show pymilvus | grep -Ei 'Name:|Version:'\n",
    "! pip3 show towhee | grep -Ei 'Name:|Version:'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa8171",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing\n",
    "\n",
    "The data is from the [Cleaned Medium Articles Dataset](https://www.kaggle.com/datasets/shiyu22chen/cleaned-medium-articles-dataset)(you can download it from Kaggle), which cleared the empty article titles in the data and conver the string title to the embeeding with Towhee [text_embedding.dpr operator](https://towhee.io/text-embedding/dpr), as you can see the `title_vector` is the embedding vectors of the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf16ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "! wget -q https://github.com/towhee-io/examples/releases/download/data/New_Medium_Data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1052d",
   "metadata": {},
   "source": [
    "## 1.1 Adding embeddings for columns\n",
    "\n",
    "The dataset is from the [Kartverket dataset metadata](https://cdn.discordapp.com/attachments/1204433663035449384/1206537816654356480/metadata_no_format.csv?ex=65dc5ee7&is=65c9e9e7&hm=3b9a88db41103ef5393294c5eaeebb60ee2229f43724cc014d4cffc92de1f384&), which contains metadata about each dataset.\n",
    "\n",
    "The strings in the columns need to be converted to vector representations (embedding) using Towhee [text_embedding.dpr operator](https://towhee.io/text-embedding/dpr). Columns containing these new embedings should contain the original column name with `_vector` at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feae67f",
   "metadata": {},
   "source": [
    "### NB In case pandas cannot read the csv, due to a delimiter parsing error\n",
    "\n",
    "Use the code below to reformat the delimiters to \"|\", and replace the excess ones that replaced the commas inside sentences with regular commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dd32417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for reformatting the delimiters to \"|\"\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def replace_delimiter(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Regular expression to match commas not inside double quotes\n",
    "    pattern = r',(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)'\n",
    "\n",
    "    # Replace the matched commas with '|'\n",
    "    new_content = re.sub(pattern, '|', content)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(new_content)\n",
    "\n",
    "# Replace this with your actual file paths\n",
    "input_file = 'metadata.csv'\n",
    "output_file = 'output_metadata_modified.csv'\n",
    "\n",
    "replace_delimiter(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a41e0667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema                   object\n",
      "uuid                     object\n",
      "id                       object\n",
      "hierarchyLevel           object\n",
      "title                    object\n",
      "datasetcreationdate      object\n",
      "abstract                 object\n",
      "keyword                  object\n",
      "geoBox                   object\n",
      "Constraints              object\n",
      "SecurityConstraints      object\n",
      "LegalConstraints         object\n",
      "temporalExtent           object\n",
      "image                    object\n",
      "responsibleParty         object\n",
      "link                     object\n",
      "metadatacreationdate     object\n",
      "productInformation       object\n",
      "parentId                float64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 16:42:07,544 - 14157082624 - dpr.py-dpr:46 - ERROR: Invalid input for the tokenizer: facebook/dpr-ctx_encoder-single-nq-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: title\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Node-text-embedding/dpr-0 runs failed, error msg: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)., Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/nodes/node.py\", line 158, in _call\n    return True, self._op(*inputs), None\n  File \"/Users/williamaredal/.towhee/operators/text-embedding/dpr/versions/main/dpr.py\", line 47, in __call__\n    raise e\n  File \"/Users/williamaredal/.towhee/operators/text-embedding/dpr/versions/main/dpr.py\", line 44, in __call__\n    input_ids = self.tokenizer(txt, return_tensors=\"pt\")[\"input_ids\"]\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2803, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2861, in _call_one\n    raise ValueError(\nValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n, Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/nodes/node.py\", line 171, in process\n    self.process_step()\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/nodes/_map.py\", line 63, in process_step\n    assert succ, msg\nAssertionError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)., Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/nodes/node.py\", line 158, in _call\n    return True, self._op(*inputs), None\n  File \"/Users/williamaredal/.towhee/operators/text-embedding/dpr/versions/main/dpr.py\", line 47, in __call__\n    raise e\n  File \"/Users/williamaredal/.towhee/operators/text-embedding/dpr/versions/main/dpr.py\", line 44, in __call__\n    input_ids = self.tokenizer(txt, return_tensors=\"pt\")[\"input_ids\"]\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2803, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2861, in _call_one\n    raise ValueError(\nValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns_to_exclude]:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing column: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     df[column \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_vector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Display the dataframe\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[45], line 22\u001b[0m, in \u001b[0;36mcompute_embeddings\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_embeddings\u001b[39m(text):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataCollection(\u001b[43membeddings_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mto_list()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvec\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/runtime_pipeline.py:159\u001b[0m, in \u001b[0;36mRuntimePipeline.__call__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Output with ordering matching the input `DataQueue`.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/runtime_pipeline.py:177\u001b[0m, in \u001b[0;36mRuntimePipeline._call\u001b[0;34m(self, profiler, tracer, trace_edges, *inputs)\u001b[0m\n\u001b[1;32m    174\u001b[0m time_profiler \u001b[38;5;241m=\u001b[39m TimeProfiler(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m profiler \u001b[38;5;28;01melse\u001b[39;00m TimeProfiler(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m graph \u001b[38;5;241m=\u001b[39m _Graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dag_repr\u001b[38;5;241m.\u001b[39mnodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dag_repr\u001b[38;5;241m.\u001b[39medges, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operator_pool, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread_pool, time_profiler, trace_edges)\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, [graph\u001b[38;5;241m.\u001b[39mtime_profiler] \u001b[38;5;28;01mif\u001b[39;00m profiler \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, [graph\u001b[38;5;241m.\u001b[39mdata_queues] \u001b[38;5;28;01mif\u001b[39;00m tracer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/runtime_pipeline.py:116\u001b[0m, in \u001b[0;36m_Graph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Union[Tuple, List]):\n\u001b[1;32m    115\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_call(inputs)\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/runtime_pipeline.py:34\u001b[0m, in \u001b[0;36m_GraphResult.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mrelease_op()\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/runtime_pipeline.py:95\u001b[0m, in \u001b[0;36m_Graph.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m             errs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39merr_msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errs:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(errs)\n\u001b[1;32m     96\u001b[0m end_edge_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mout_edges[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     97\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queues[end_edge_num]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Node-text-embedding/dpr-0 runs failed, error msg: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)., Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/nodes/node.py\", line 158, in _call\n    return True, self._op(*inputs), None\n  File \"/Users/williamaredal/.towhee/operators/text-embedding/dpr/versions/main/dpr.py\", line 47, in __call__\n    raise e\n  File \"/Users/williamaredal/.towhee/operators/text-embedding/dpr/versions/main/dpr.py\", line 44, in __call__\n    input_ids = self.tokenizer(txt, return_tensors=\"pt\")[\"input_ids\"]\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2803, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2861, in _call_one\n    raise ValueError(\nValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n, Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/nodes/node.py\", line 171, in process\n    self.process_step()\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/nodes/_map.py\", line 63, in process_step\n    assert succ, msg\nAssertionError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)., Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/towhee/runtime/nodes/node.py\", line 158, in _call\n    return True, self._op(*inputs), None\n  File \"/Users/williamaredal/.towhee/operators/text-embedding/dpr/versions/main/dpr.py\", line 47, in __call__\n    raise e\n  File \"/Users/williamaredal/.towhee/operators/text-embedding/dpr/versions/main/dpr.py\", line 44, in __call__\n    input_ids = self.tokenizer(txt, return_tensors=\"pt\")[\"input_ids\"]\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2803, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2861, in _call_one\n    raise ValueError(\nValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from towhee import pipe, ops, DataCollection\n",
    "\n",
    "df = pd.read_csv('output_metadata_modified.csv', delimiter='|', encoding='latin-1')\n",
    "\n",
    "# Recasts 'title' column to string\n",
    "recast_to_string = ['title', 'uuid']\n",
    "df[recast_to_string] = df[recast_to_string].astype('object')\n",
    "print(df.dtypes)\n",
    "\n",
    "\n",
    "# Pipe converting text to embeddings (vectors)\n",
    "embeddings_pipe = (\n",
    "    pipe.input('text')\n",
    "        .map('text', 'vec', ops.text_embedding.dpr(model_name='facebook/dpr-ctx_encoder-single-nq-base'))\n",
    "        .output('text', 'vec')\n",
    ")\n",
    "\n",
    "df = df.iloc[:10]\n",
    "# Function to compute embeddings for a single text\n",
    "def compute_embeddings(text):\n",
    "    return DataCollection(embeddings_pipe(text)).to_list()[0]['vec']\n",
    "\n",
    "# Process each column (excluding 'uuid' and 'id') and create new columns for embeddings\n",
    "columns_to_exclude = ['schema', 'uuid', 'id', 'datasetcreationdate', 'image', 'parentId']\n",
    "for column in [col for col in df.columns if col not in columns_to_exclude]:\n",
    "    print(f\"Processing column: {column}\")\n",
    "    df[column + '_vector'] = df[column].apply(compute_embeddings)\n",
    "'''\n",
    "'''\n",
    "\n",
    "# Display the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('New_Medium_Data.csv', converters={'title_vector': lambda x: eval(x)})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c89e09d",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "The next step is to get the text embedding, and then insert all the extracted embedding vectors into Milvus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107e24c",
   "metadata": {},
   "source": [
    "### Create Milvus Collection\n",
    "\n",
    "We need to create a collection in Milvus first, which contains multiple fields of `id`, `title`, `title_vector`, `link`, `reading_time`, `publication`, `claps` and `responses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a85c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "server_host = 'ebjerk.no'\n",
    "connections.connect(host=server_host, port='19530')\n",
    "\n",
    "def create_milvus_collection(collection_name, dim):\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "            FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=500),   \n",
    "            FieldSchema(name=\"title_vector\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n",
    "            FieldSchema(name=\"link\", dtype=DataType.VARCHAR, max_length=500),\n",
    "            FieldSchema(name=\"reading_time\", dtype=DataType.INT64),\n",
    "            FieldSchema(name=\"publication\", dtype=DataType.VARCHAR, max_length=500),\n",
    "            FieldSchema(name=\"claps\", dtype=DataType.INT64),\n",
    "            FieldSchema(name=\"responses\", dtype=DataType.INT64)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description='search text')\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "    \n",
    "    index_params = {\n",
    "        'metric_type': \"L2\",\n",
    "        'index_type': \"IVF_FLAT\",\n",
    "        'params': {\"nlist\": 2048}\n",
    "    }\n",
    "    collection.create_index(field_name='title_vector', index_params=index_params)\n",
    "    return collection\n",
    "\n",
    "collection = create_milvus_collection('search_article_in_medium', 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5f0d2",
   "metadata": {},
   "source": [
    "### Data to Milvus\n",
    "\n",
    "\n",
    "Towhee supports reading df data through the `from_df` interface, and then we need to convert the `title_vector` column in the data to a two-dimensional list in float format, and then insert all the fields into Milvus, each field inserted into Milvus corresponds to one Collection fields created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d52c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee import ops, pipe, DataCollection\n",
    "\n",
    "insert_pipe = (pipe.input('df')\n",
    "                   .flat_map('df', 'data', lambda df: df.values.tolist())\n",
    "                   .map('data', 'res', ops.ann_insert.milvus_client(host=server_host, \n",
    "                                                                    port='19530',\n",
    "                                                                    collection_name='search_article_in_medium'))\n",
    "                   .output('res')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = insert_pipe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c149d",
   "metadata": {},
   "source": [
    "We need to call `collection.load()` to load the data after inserting the data, then run `collection.num_entities` to get the number of vectors in the collection. We will see the number of vectors is 5979, and we have successfully load the data to Milvus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.load()\n",
    "collection.num_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea34c1f4",
   "metadata": {},
   "source": [
    "## 3. Search embedding title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8662a",
   "metadata": {},
   "source": [
    "### Search one text in Milvus\n",
    "\n",
    "\n",
    "The retrieval process also to generate the text embedding of the query text, then search for similar vectors in Milvus, and finally return the result, which contains `id`(primary_key) and `score`. For example, we can search for \"funny python demo\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "search_pipe = (pipe.input('query')\n",
    "                    .map('query', 'vec', ops.text_embedding.dpr(model_name=\"facebook/dpr-ctx_encoder-single-nq-base\"))\n",
    "                    .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
    "                    .flat_map('vec', ('id', 'score'), ops.ann_search.milvus_client(host=server_host, \n",
    "                                                                                   port='19530',\n",
    "                                                                                   collection_name='search_article_in_medium'))  \n",
    "                    .output('query', 'id', 'score')\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbca936",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_pipe('funny python demo')\n",
    "DataCollection(res).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5dc7f",
   "metadata": {},
   "source": [
    "### Search multi text in Milvus\n",
    "\n",
    "We can also retrieve multiple pieces of data, for example we can specify the array(['funny python demo', 'AI in data analysis']) to search in batch, which will be retrieved in Milvus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a97384",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_pipe.batch(['funny python demo', 'AI in data analysis'])\n",
    "for re in res:\n",
    "    DataCollection(re).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835da6f",
   "metadata": {},
   "source": [
    "### Search text and return multi fields\n",
    "\n",
    "If we want to return more information when retrieving, we can set the `output_fields` parameter in [ann_search.milvus operator](https://towhee.io/ann-search/milvus). For example, in addition to `id` and `score`, we can also return `title`, `link`, `claps`, `reading_time`, `and response`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c736c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_pipe1 = (pipe.input('query')\n",
    "                    .map('query', 'vec', ops.text_embedding.dpr(model_name=\"facebook/dpr-ctx_encoder-single-nq-base\"))\n",
    "                    .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
    "                    .flat_map('vec', ('id', 'score', 'title'), ops.ann_search.milvus_client(host=server_host, \n",
    "                                                                                   port='19530',\n",
    "                                                                                   collection_name='search_article_in_medium',\n",
    "                                                                                   output_fields=['title']))  \n",
    "                    .output('query', 'id', 'score', 'title')\n",
    "               )\n",
    "\n",
    "res = search_pipe1('funny python demo')\n",
    "DataCollection(res).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# milvus search with multi outpt fields\n",
    "search_pipe2 = (pipe.input('query')\n",
    "                    .map('query', 'vec', ops.text_embedding.dpr(model_name=\"facebook/dpr-ctx_encoder-single-nq-base\"))\n",
    "                    .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
    "                    .flat_map('vec', ('id', 'score', 'title', 'link', 'reading_time', 'publication', 'claps', 'responses'), \n",
    "                                       ops.ann_search.milvus_client(host=server_host, \n",
    "                                                                    port='19530',\n",
    "                                                                    collection_name='search_article_in_medium',\n",
    "                                                                    output_fields=['title', 'link', 'reading_time', 'publication', 'claps', 'responses'], \n",
    "                                                                    limit=5))  \n",
    "                    .output('query', 'id', 'score', 'title', 'link', 'reading_time', 'publication', 'claps', 'responses')\n",
    "               )\n",
    "\n",
    "res = search_pipe2('funny python demo')\n",
    "DataCollection(res).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff903e3",
   "metadata": {},
   "source": [
    "### Search text with some expr\n",
    "\n",
    "\n",
    "In addition, we can also set some expressions for retrieval. For example, we can specify that the beginning of the article is an article in Python by setting expr='title like \"Python%\"':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_pipe3 = (pipe.input('query')\n",
    "                    .map('query', 'vec', ops.text_embedding.dpr(model_name=\"facebook/dpr-ctx_encoder-single-nq-base\"))\n",
    "                    .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
    "                    .flat_map('vec', ('id', 'score', 'title', 'link', 'reading_time', 'publication', 'claps', 'responses'), \n",
    "                                       ops.ann_search.milvus_client(host=server_host, \n",
    "                                                                    port='19530',\n",
    "                                                                    collection_name='search_article_in_medium',\n",
    "                                                                    expr='title like \"Python%\"',\n",
    "                                                                    output_fields=['title', 'link', 'reading_time', 'publication', 'claps', 'responses'], \n",
    "                                                                    limit=5))  \n",
    "                    .output('query', 'id', 'score', 'title', 'link', 'reading_time', 'publication', 'claps', 'responses')\n",
    "               )\n",
    "\n",
    "res = search_pipe3('funny python demo')\n",
    "DataCollection(res).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50699084",
   "metadata": {},
   "source": [
    "## 4. Query data in Milvus\n",
    "\n",
    "We have done the text retrieval process before, and we can get articles such as \"Python coding for kids - getting ready for an adventure\" by retrieving \"fun python demos\".\n",
    "\n",
    "We can also do a simple query on the data, we need to set `expr` and `output_fields` with the `collection.query` interface, for example, we can filter out articles with faults greater than 300 and reading time less than 15 minutes, and submitted to TDS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(\n",
    "  expr = 'claps > 3000 && reading_time < 15 && publication like \"Towards Data Science%\"', \n",
    "  output_fields = ['id', 'title', 'link', 'reading_time', 'publication', 'claps', 'responses'],\n",
    "  consistency_level='Strong'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f67c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
